{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "def prepare_model_and_tokenizer(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,  # Use half precision\n",
    "        device_map=\"auto\"  # Automatically handle model placement\n",
    "    )\n",
    "    return model, tokenizer\n",
    "\n",
    "def prepare_dataset(dataset_name, tokenizer):\n",
    "    dataset = load_dataset(dataset_name)\n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "    \n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=dataset[\"train\"].column_names\n",
    "    )\n",
    "    \n",
    "    return tokenized_dataset\n",
    "\n",
    "def main():\n",
    "    model_name = \"mistralai/Mistral-7B-v0.1\"  # Or any other model\n",
    "    dataset_name = \"your_dataset_name\"\n",
    "    \n",
    "    model, tokenizer = prepare_model_and_tokenizer(model_name)\n",
    "    \n",
    "    tokenized_dataset = prepare_dataset(dataset_name, tokenizer)\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=2e-5,\n",
    "        fp16=True,\n",
    "        save_steps=500,\n",
    "        logging_steps=100,\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
